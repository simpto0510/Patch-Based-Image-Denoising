{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lmdb\n!pip install torchviz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport os\nimport cv2\nimport glob\nfrom torchvision import models\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils import spectral_norm\nimport kornia\nfrom kornia.losses import ssim as kornia_ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim_sk\nfrom torch.utils.data.sampler import RandomSampler\n\n\n# Enable benchmark mode in cudnn (optional optimization)\ntorch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport os\nimport glob\nimport cv2\nimport random\nimport time\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nimport lmdb\n\n############################################\n# 1. File-Based Dataset for SIDD Training/Validation Patches\n############################################\n\nclass SIDDFileDataset(Dataset):\n    \"\"\"\n    A dataset class for SIDD training/validation patches stored as individual PNG files.\n    Assumes that noisy patches and ground truth patches are stored in separate directories\n    with matching filenames (e.g., '00000001.png' in both directories).\n    \"\"\"\n    def __init__(self, noisy_dir, gt_dir, transform=None):\n        self.noisy_dir = noisy_dir\n        self.gt_dir = gt_dir\n        self.transform = transform\n        \n        # List and sort all PNG files from the two directories.\n        self.noisy_files = sorted(glob.glob(os.path.join(noisy_dir, '*.png')))\n        self.gt_files = sorted(glob.glob(os.path.join(gt_dir, '*.png')))\n        \n        if len(self.noisy_files) != len(self.gt_files):\n            raise ValueError(\"Mismatch between number of noisy and GT images. \"\n                             f\"Found {len(self.noisy_files)} noisy and {len(self.gt_files)} GT images.\")\n        \n    def __len__(self):\n        return len(self.noisy_files)\n    \n    def __getitem__(self, idx):\n        # Get file paths for the corresponding noisy and GT images.\n        noisy_file = self.noisy_files[idx]\n        gt_file = self.gt_files[idx]\n        \n        # Read images using OpenCV.\n        noisy_img = cv2.imread(noisy_file, cv2.IMREAD_COLOR)\n        gt_img = cv2.imread(gt_file, cv2.IMREAD_COLOR)\n        if noisy_img is None or gt_img is None:\n            raise RuntimeError(f\"Failed to read images: {noisy_file} or {gt_file}\")\n        \n        # Convert from BGR (OpenCV default) to RGB.\n        noisy_img = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)\n        gt_img = cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB)\n        \n        # Convert from HWC to CHW format and normalize to [0, 1].\n        noisy_img = torch.from_numpy(np.transpose(noisy_img, (2, 0, 1))).float() / 255.0\n        gt_img = torch.from_numpy(np.transpose(gt_img, (2, 0, 1))).float() / 255.0\n        \n        if self.transform is not None:\n            noisy_img, gt_img = self.transform(noisy_img, gt_img)\n            \n        return noisy_img, gt_img\n\n############################################\n# 2. Custom Sampler: Randomly Subsample a Fixed Number of Patches Each Epoch\n############################################\n\nclass SubsetRandomSampler(Sampler):\n    \"\"\"\n    A sampler that randomly selects a fixed number of indices (num_samples)\n    from the dataset for each epoch.\n    \"\"\"\n    def __init__(self, data_source, num_samples):\n        self.data_source = data_source\n        self.num_samples = num_samples\n\n    def __iter__(self):\n        # Create a list of indices and use current time to seed the random module\n        indices = list(range(len(self.data_source)))\n        random.seed(time.time())\n        random.shuffle(indices)\n        return iter(indices[:self.num_samples])\n\n    def __len__(self):\n        return self.num_samples\n\n############################################\n# 3. Worker Initialization Function\n############################################\n\ndef worker_init_fn(worker_id):\n    \"\"\"\n    Worker initialization function to ensure different random seeds across workers.\n    \"\"\"\n    seed = int(time.time()) + worker_id\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n############################################\n# 4. Example Usage for Training and Validation\n############################################\n\n# Parameters\nbatch_size = 8\nnum_samples_per_epoch = 320\n\nif __name__ == '__main__':\n    # -------------------------------\n    # Training\n    # -------------------------------\n    # Use the modified training data directories.\n    train_noisy_dir = '/kaggle/input/sagluuuu/train_input'\n    train_gt_dir = '/kaggle/input/sagluuuu/train_gt'\n    \n    # Create an instance of the training dataset.\n    train_dataset = SIDDFileDataset(train_noisy_dir, train_gt_dir)\n    \n    # Create the custom sampler.\n    sampler = SubsetRandomSampler(train_dataset, num_samples_per_epoch)\n    \n    # Create a DataLoader with the custom sampler and worker initialization.\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,          # Adjust batch size as needed.\n        sampler=sampler,\n        num_workers=4,                  # Adjust based on available CPU cores.\n        pin_memory=True,\n        persistent_workers=True,        # Optional: reduces worker restart overhead.\n        worker_init_fn=worker_init_fn\n    )\n    \n    # Iterate over one training epoch.\n    print(\"TRAINING EPOCH:\")\n    for batch_idx, (input_imgs, gt_imgs) in enumerate(train_loader):\n        print(f\"Train Batch {batch_idx}: Input shape {input_imgs.shape}, GT shape {gt_imgs.shape}\")\n        # For demonstration purposes, break after one batch.\n        break\n\n    # -------------------------------\n    # Validation\n    # -------------------------------\n    # The LMDB-based validation dataset is assumed to be unchanged.\n    class SIDDValLMDB(Dataset):\n        \"\"\"\n        A dataset class for SIDD validation data stored in LMDBs.\n        Each key corresponds to a pre-cropped noisy/clean patch (encoded as PNG bytes).\n        \"\"\"\n        def __init__(self, input_lmdb_path, gt_lmdb_path, transform=None):\n            self.input_env = lmdb.open(input_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n            self.gt_env = lmdb.open(gt_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n            self.transform = transform\n            with self.input_env.begin() as txn:\n                self.keys = [key for key, _ in txn.cursor() if key != b'length']\n            self.keys = sorted(self.keys)\n            self.length = len(self.keys)\n\n        def __len__(self):\n            return self.length\n\n        def __getitem__(self, idx):\n            key = self.keys[idx]\n            with self.input_env.begin() as txn:\n                input_bytes = txn.get(key)\n            with self.gt_env.begin() as txn:\n                gt_bytes = txn.get(key)\n            if input_bytes is None or gt_bytes is None:\n                raise KeyError(f\"Key {key} not found in one of the LMDBs.\")\n            # Decode PNG bytes to image arrays (BGR)\n            input_img = cv2.imdecode(np.frombuffer(input_bytes, np.uint8), cv2.IMREAD_COLOR)\n            gt_img = cv2.imdecode(np.frombuffer(gt_bytes, np.uint8), cv2.IMREAD_COLOR)\n            if input_img is None or gt_img is None:\n                raise RuntimeError(f\"Failed to decode images for key {key}.\")\n            # Convert from BGR to RGB\n            input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n            gt_img = cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB)\n            # Convert to torch tensors and scale to [0,1]\n            input_img = torch.from_numpy(np.transpose(input_img, (2, 0, 1))).float() / 255.0\n            gt_img = torch.from_numpy(np.transpose(gt_img, (2, 0, 1))).float() / 255.0\n            if self.transform is not None:\n                input_img, gt_img = self.transform(input_img, gt_img)\n            return input_img, gt_img\n\n    # Paths for validation LMDBs\n    val_input_lmdb = '/kaggle/input/smartphone-image-denoising-dataset/SIDD-val-lmdb/SIDD/val/input_crops.lmdb'\n    val_gt_lmdb = '/kaggle/input/smartphone-image-denoising-dataset/SIDD-val-lmdb/SIDD/val/gt_crops.lmdb'\n    val_dataset = SIDDValLMDB(val_input_lmdb, val_gt_lmdb)\n    \n    # Create a DataLoader for validation.\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n        persistent_workers=True,\n        worker_init_fn=worker_init_fn\n    )\n    \n    # Iterate over one validation batch.\n    print(\"\\nVALIDATION BATCH:\")\n    for input_imgs, gt_imgs in val_loader:\n        print(\"Validation batch shapes:\", input_imgs.shape, gt_imgs.shape)\n        break","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generator\n","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# --------------------------------------------------------------------------\n# 1) Utility Functions: Splitting and Merging Patches\n# --------------------------------------------------------------------------\ndef split_into_patches(img, patch_size, overlap=0):\n    \"\"\"\n    Splits a batch of images into overlapping or non-overlapping patches.\n\n    Args:\n        img (torch.Tensor): shape (B, C, H, W).\n        patch_size (int): the spatial size of each patch.\n        overlap (int): overlap in pixels for adjacent patches. If > 0,\n                       patches will overlap, requiring careful merging.\n\n    Returns:\n        (torch.Tensor):\n            Patches of shape (B, N, C, patch_size, patch_size),\n            where N = #patches per image (horiz_patches * vert_patches).\n    \"\"\"\n    B, C, H, W = img.shape\n    stride = patch_size - overlap\n\n    patches_out = []\n    vertical_count = math.ceil((H - overlap) / stride)\n    horizontal_count = math.ceil((W - overlap) / stride)\n\n    for b_i in range(B):\n        these_patches = []\n        for v in range(vertical_count):\n            for h in range(horizontal_count):\n                top = v * stride\n                left = h * stride\n                bottom = min(top + patch_size, H)\n                right = min(left + patch_size, W)\n\n                patch = img[b_i:b_i+1, :, top:bottom, left:right]\n\n                # If near boundaries, patch might be smaller. We pad so every patch is the same size.\n                pad_bottom = patch_size - (bottom - top)\n                pad_right = patch_size - (right - left)\n                if pad_bottom > 0 or pad_right > 0:\n                    patch = F.pad(patch, (0, pad_right, 0, pad_bottom), mode='replicate')\n\n                these_patches.append(patch)\n        # Stack along patch dimension => (N, C, patch_size, patch_size)\n        these_patches = torch.cat(these_patches, dim=0)\n        # Keep a batch dimension => (1, N, C, patch_size, patch_size)\n        these_patches = these_patches.unsqueeze(0)\n        patches_out.append(these_patches)\n\n    return torch.cat(patches_out, dim=0)\n\n\ndef merge_from_patches(patches, out_shape, patch_size, overlap=0):\n    \"\"\"\n    Merges patches (potentially overlapping) back into a single image.\n\n    Args:\n        patches (torch.Tensor): shape (B, N, C, patch_size, patch_size).\n        out_shape (tuple): the original image shape (B, C, H, W).\n        patch_size (int): patch dimension used in splitting.\n        overlap (int): overlap in pixels.\n\n    Returns:\n        (torch.Tensor):\n            A tensor of shape (B, C, H, W) with overlapped regions blended.\n    \"\"\"\n    B, N, C, _, _ = patches.shape\n    _, _, H, W = out_shape\n    stride = patch_size - overlap\n\n    vertical_count = math.ceil((H - overlap) / stride)\n    horizontal_count = math.ceil((W - overlap) / stride)\n\n    # We'll reconstruct each image in the batch independently\n    merged_imgs = []\n    idx_start = 0\n\n    for b_i in range(B):\n        # Large accumulators for pixel values and blending weights\n        accumulator = torch.zeros((C, H, W), device=patches.device)\n        weight_map = torch.zeros((C, H, W), device=patches.device)\n\n        # patches for image b_i: shape => (N, C, patch_size, patch_size)\n        # Assume N == vertical_count * horizontal_count\n        n_patches = vertical_count * horizontal_count\n        b_patches = patches[b_i, :n_patches, :, :, :]\n\n        patch_idx = 0\n        for v in range(vertical_count):\n            for h in range(horizontal_count):\n                top = v * stride\n                left = h * stride\n                bottom = top + patch_size\n                right = left + patch_size\n\n                patch = b_patches[patch_idx]\n                patch_idx += 1\n\n                # If we padded earlier, we might need to clip here\n                patch_height = min(patch_size, H - top)\n                patch_width = min(patch_size, W - left)\n                patch_cropped = patch[:, :patch_height, :patch_width]\n\n                accumulator[:, top:top+patch_height, left:left+patch_width] += patch_cropped\n                weight_map[:, top:top+patch_height, left:left+patch_width] += 1.0\n\n        # average in overlapped zones\n        weight_map[weight_map == 0] = 1e-9\n        final_img = accumulator / weight_map\n        merged_imgs.append(final_img.unsqueeze(0))\n\n    # Combine all batch items\n    return torch.cat(merged_imgs, dim=0)\n\n\n# --------------------------------------------------------------------------\n# 2) CNN or Small Denoiser Blocks\n# --------------------------------------------------------------------------\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    A small residual block with two 3x3 conv layers. Useful for refining patches.\n    \"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        identity = x\n        out = self.activation(self.conv1(x))\n        out = self.conv2(out)\n        return self.activation(out + identity)\n\n\nclass MiniPatchDenoiser(nn.Module):\n    \"\"\"\n    At the smallest fractal level, do a final 'pixel-level' or small-patch cleanup.\n    For brevity, we define it as 2 residual blocks.\n\n    If you want something more advanced, you can chain more blocks or\n    do per-pixel transformations, etc.\n    \"\"\"\n    def __init__(self, in_channels, hidden_channels=32):\n        super().__init__()\n        self.initial = nn.Conv2d(in_channels, hidden_channels, 3, padding=1)\n        self.res1 = ResidualBlock(hidden_channels)\n        self.res2 = ResidualBlock(hidden_channels)\n        self.final = nn.Conv2d(hidden_channels, in_channels, 3, padding=1)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        z = F.relu(self.initial(x))\n        z = self.res1(z)\n        z = self.res2(z)\n        out = self.final(z)\n        # Optionally do a residual add or direct output\n        out = x + out  # residual style final\n        return out\n\n\n# --------------------------------------------------------------------------\n# 3) Recursive \"FractalBlock\" - a building block that calls an optional child\n# --------------------------------------------------------------------------\nclass FractalDenoiseBlock(nn.Module):\n    \"\"\"\n    One fractal level that:\n      - Splits the image into patches\n      - If child is not None, each patch is recursively denoised at a smaller scale\n      - Then merges patches\n      - Applies a local CNN to refine the merged result\n    \"\"\"\n    def __init__(\n        self,\n        patch_size,\n        overlap,\n        in_channels,\n        hidden_channels,\n        child_block=None\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n        self.overlap = overlap\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n\n        # A local refining CNN or residual stack for this scale\n        self.local_cnn = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, in_channels, kernel_size=3, padding=1),\n        )\n\n        # If child is None => final fractal level (we do a mini patch denoiser).\n        # Otherwise we recursively call child on patches.\n        if child_block is not None:\n            self.child_block = child_block\n        else:\n            self.child_block = MiniPatchDenoiser(in_channels, hidden_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Denoising logic at this fractal level:\n          1. If child_block is a FractalDenoiseBlock, we:\n             - split x into patches\n             - run each patch through child_block\n             - merge them\n          2. Then apply local_cnn (or final mini denoiser) to refine.\n        \"\"\"\n        # if child_block is still a block, we do recursion on patches\n        if isinstance(self.child_block, FractalDenoiseBlock):\n            # Step 1: split image => patches\n            patches = split_into_patches(\n                x, patch_size=self.patch_size, overlap=self.overlap\n            )\n            B, N, C, Hp, Wp = patches.shape\n\n            # Flatten for processing\n            patches = patches.view(B*N, C, Hp, Wp)\n            # Recursively denoise each patch at smaller fractal level\n            refined_patches = self.child_block(patches)\n            # Reshape back\n            refined_patches = refined_patches.view(B, N, C, Hp, Wp)\n\n            # Merge\n            x_merged = merge_from_patches(\n                refined_patches, x.shape, self.patch_size, self.overlap\n            )\n            # Then local refine\n            return self.local_cnn(x_merged)\n        else:\n            # If child_block is our final \"MiniPatchDenoiser\", we just run it on x\n            return self.child_block(x)\n\n\n# --------------------------------------------------------------------------\n# 4) Top-level \"FractalDenoiseGenerator\" Class\n# --------------------------------------------------------------------------\nclass FractalDenoiseGenerator(nn.Module):\n    \"\"\"\n    Recursively builds fractal-based denoising modules for multi-scale patch refinement.\n    Similar to \"FractalGen\" in structure:\n      - We define multiple fractal levels\n      - Each level calls the next recursively, until final level is a mini patch denoiser\n    \"\"\"\n    def __init__(\n        self,\n        patch_sizes,\n        overlap,\n        in_channels=3,\n        hidden_channels=32\n    ):\n        \"\"\"\n        Args:\n            patch_sizes (list): e.g. [16, 4, 1], from coarse to fine patches\n            overlap (int): overlap in patches\n            in_channels (int): e.g. 3 for RGB\n            hidden_channels (int): internal channels for local refinements\n        \"\"\"\n        super().__init__()\n        # We'll build from the end (smallest patch) up to largest\n        child = None\n        # reversed(patch_sizes) => from small to large\n        # final patch size => final fractal block is actually a mini denoiser instead\n        for idx, psize in enumerate(reversed(patch_sizes)):\n            if child is None:\n                # final block\n                block = FractalDenoiseBlock(\n                    patch_size=psize,\n                    overlap=overlap,\n                    in_channels=in_channels,\n                    hidden_channels=hidden_channels,\n                    child_block=None\n                )\n            else:\n                block = FractalDenoiseBlock(\n                    patch_size=psize,\n                    overlap=overlap,\n                    in_channels=in_channels,\n                    hidden_channels=hidden_channels,\n                    child_block=child\n                )\n            child = block\n\n        # 'child' is now the chain from smallest to largest patch\n        # The largest patch block is the root we will call\n        self.root_block = child\n\n    def forward(self, noisy_img):\n        \"\"\"\n        Orchestrates fractal recursion from large patch scale down to final pixel-level cleanup.\n\n        Args:\n            noisy_img: shape (B, C, H, W)\n\n        Returns:\n            denoised_img: shape (B, C, H, W)\n        \"\"\"\n        return self.root_block(noisy_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple, List, Optional, Dict, Union\n\n# Configuration\nCONFIG = {\n    'IMAGE_SIZE': 256,            # Input image size\n    'NUM_CHANNELS_IN': 3,         # Number of input channels (RGB)\n    'NUM_CHANNELS_OUT': 3,        # Number of output channels (same as input for denoising)\n    'INIT_FEATURES': 64,          # Initial number of features\n    'DEPTH_LEVELS': 5,            # Number of downsampling/upsampling steps\n    'EXPANSION_LEVEL': 3,         # Fractal block recursion depth\n    'ACTIVATION': 'leaky_relu',   # Activation function - leaky_relu is better for GANs\n    'USE_BATCHNORM': True,        # Use batch normalization\n}\n\n\nclass FractalConvBlock(nn.Module):\n    \"\"\"\n    Fractal Convolutional Block implementing equations (2) and (3) from the FractalSpiNet paper.\n    \n    This block recursively expands convolutional operations into fractal patterns,\n    allowing for more complex feature extraction with fewer parameters.\n    \n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        expansion_level (int): Fractal recursion depth\n        use_bn (bool): Whether to use batch normalization\n        activation (str): Activation function to use ('relu' or 'leaky_relu')\n        kernel_size (int): Kernel size for convolutions\n    \"\"\"\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        expansion_level: int = 1,\n        use_bn: bool = True,\n        activation: str = 'relu',\n        kernel_size: int = 3\n    ):\n        super(FractalConvBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.expansion_level = expansion_level\n        self.use_bn = use_bn\n        self.activation = activation\n        self.kernel_size = kernel_size\n        \n        # Define activation function\n        if activation == 'relu':\n            self.act = nn.ReLU(inplace=True)\n        elif activation == 'leaky_relu':\n            self.act = nn.LeakyReLU(0.2, inplace=True)\n        else:\n            raise ValueError(f\"Unsupported activation: {activation}\")\n        \n        # Base case: eq. (2) - simple convolution with optional BN and activation\n        if expansion_level == 1:\n            self.conv = nn.Conv2d(\n                in_channels, \n                out_channels, \n                kernel_size=kernel_size, \n                padding=kernel_size//2,\n                bias=not use_bn\n            )\n            \n            if use_bn:\n                self.bn = nn.BatchNorm2d(out_channels)\n            \n        # Recursive case: eq. (3) - fractal expansion\n        else:\n            # First branch: f_{c-1}(z)\n            self.branch1 = FractalConvBlock(\n                in_channels, \n                out_channels, \n                expansion_level - 1,\n                use_bn,\n                activation,\n                kernel_size\n            )\n            \n            # Second branch: f_{c-1}(f_{c-1}(z))\n            self.branch2 = FractalConvBlock(\n                out_channels, \n                out_channels, \n                expansion_level - 1,\n                use_bn,\n                activation,\n                kernel_size\n            )\n            \n            # 1x1 convolution to merge channels if needed\n            self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through the fractal block.\"\"\"\n        # Base case: single convolution (eq. 2)\n        if self.expansion_level == 1:\n            out = self.conv(x)\n            if self.use_bn:\n                out = self.bn(out)\n            out = self.act(out)\n            return out\n        \n        # Recursive case: fractal expansion (eq. 3)\n        else:\n            # First branch\n            branch1_out = self.branch1(x)\n            \n            # Second branch (takes output of first branch as input)\n            branch2_out = self.branch2(branch1_out)\n            \n            # Residual connection\n            skip = self.skip_conv(x)\n            \n            # Combine with residual\n            return branch2_out + skip\n\n\n\n# Multiscale PatchGAN Discriminator with Fractal Blocks\nclass Discriminator(nn.Module):\n    \"\"\"\n    Multiscale PatchGAN Discriminator network for conditional GAN training.\n    Uses fractal blocks for feature extraction at multiple scales.\n    \n    Args:\n        in_channels (int): Number of input channels per image\n        init_features (int): Initial number of feature channels\n        depth_levels (int): Number of downsampling steps\n        expansion_level (int): Fractal recursion depth\n        use_bn (bool): Whether to use batch normalization\n        activation (str): Activation function to use\n    \"\"\"\n    def __init__(\n        self,\n        in_channels: int = CONFIG['NUM_CHANNELS_IN'],\n        init_features: int = 64,\n        depth_levels: int = 3,\n        expansion_level: int = 2,  # Fractal recursion depth\n        use_bn: bool = True,\n        activation: str = 'leaky_relu'\n    ):\n        super(Discriminator, self).__init__()\n        \n        # For conditional GAN, we input both noisy and clean/fake images\n        # So we double the input channels\n        combined_channels = in_channels * 2\n        \n        # Initial feature extraction\n        self.initial_conv = nn.Sequential(\n            nn.Conv2d(combined_channels, init_features, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        \n        # Multiscale feature extraction with fractal blocks\n        self.scales = nn.ModuleList()\n        in_feat = init_features\n        \n        # Create different scale pathways\n        for i in range(depth_levels):\n            out_feat = in_feat * 2\n            \n            # Fractal block for feature extraction at this scale\n            fractal_block = FractalConvBlock(\n                in_feat,\n                out_feat,\n                expansion_level=expansion_level,\n                use_bn=use_bn,\n                activation=activation,\n                kernel_size=3\n            )\n            \n            # Downsampling after fractal block\n            down_block = nn.Sequential(\n                fractal_block,\n                nn.AvgPool2d(kernel_size=2, stride=2)\n            )\n            \n            # PatchGAN output for this scale\n            output_layer = nn.Conv2d(out_feat, 1, kernel_size=4, stride=1, padding=1)\n            \n            # Add scale path to module list\n            self.scales.append(nn.ModuleDict({\n                'down': down_block,\n                'output': output_layer\n            }))\n            \n            in_feat = out_feat\n    \n    def forward(self, condition_img: torch.Tensor, target_img: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"\n        Forward pass through the multiscale discriminator.\n        \n        Args:\n            condition_img: The conditional input image (noisy image)\n            target_img: The target image (real clean or generated fake)\n            \n        Returns:\n            List of discriminator outputs at different scales\n        \"\"\"\n        # Concatenate along channel dimension for conditional input\n        x = torch.cat([condition_img, target_img], dim=1)\n        \n        # Initial features\n        features = self.initial_conv(x)\n        outputs = []\n        \n        # Process through each scale\n        for scale in self.scales:\n            features = scale['down'](features)\n            outputs.append(scale['output'](features))\n        \n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss and utils","metadata":{}},{"cell_type":"code","source":"# ============================\n#  Visualization Utility\n# ============================\ndef show_images_from_batches(generator, dataloader, device, num_images=5):\n    generator.eval()\n    images_shown = 0\n    plt.figure(figsize=(15, 5 * num_images), dpi=300)\n    for noisy_imgs, clean_imgs in dataloader:\n        if images_shown >= num_images:\n            break\n        noisy_imgs = noisy_imgs.to(device)\n        with torch.no_grad():\n            denoised_imgs = generator(noisy_imgs).cpu().numpy()\n        clean_imgs = clean_imgs.cpu().numpy()\n        noisy_imgs = noisy_imgs.cpu().numpy()\n        for idx in range(noisy_imgs.shape[0]):\n            if images_shown >= num_images:\n                break\n            real_img = np.clip(clean_imgs[idx].transpose(1, 2, 0), 0, 1)\n            denoised_img = np.clip(denoised_imgs[idx].transpose(1, 2, 0), 0, 1)\n            noisy_img = np.clip(noisy_imgs[idx].transpose(1, 2, 0), 0, 1)\n            plt.subplot(num_images, 3, 3 * images_shown + 1)\n            plt.imshow(noisy_img)\n            plt.title(f\"Noisy Image {images_shown+1}\")\n            plt.axis(\"off\")\n            plt.subplot(num_images, 3, 3 * images_shown + 2)\n            plt.imshow(denoised_img)\n            plt.title(f\"Denoised Image {images_shown+1}\")\n            plt.axis(\"off\")\n            plt.subplot(num_images, 3, 3 * images_shown + 3)\n            plt.imshow(real_img)\n            plt.title(f\"Clean Image {images_shown+1}\")\n            plt.axis(\"off\")\n            images_shown += 1\n    plt.tight_layout()\n    plt.show()\n\n\n# =====================\n#  Checkpoint Utilities\n# =====================\ndef save_best_checkpoint(generator, critic, optimizer_G, optimizer_D,\n                         epoch, loss_history, best_ssim, best_psnr, filepath):\n    \"\"\"\n    Saves the state of the models and optimizers when a new best metric is achieved.\n    \"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'critic_state_dict': critic.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'loss_history': loss_history,\n        'best_ssim': best_ssim,\n        'best_psnr': best_psnr\n    }\n    torch.save(checkpoint, filepath)\n    print(f\"Best checkpoint saved at '{filepath}' \"\n          f\"with SSIM: {best_ssim:.4f}, PSNR: {best_psnr:.2f}\")\n\n\ndef load_checkpoint(generator, critic, optimizer_G, optimizer_D, filepath, device):\n    \"\"\"\n    Loads the state of the models and optimizers from a checkpoint file.\n    \"\"\"\n    checkpoint = torch.load(filepath, map_location=device)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    critic.load_state_dict(checkpoint['critic_state_dict'])\n    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n    epoch = checkpoint['epoch']\n    loss_history = checkpoint['loss_history']\n    print(f\"Checkpoint loaded from '{filepath}' (Epoch {epoch})\")\n    return epoch, loss_history\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List\n\n# ======================\n# MS-SSIM Loss\n# ======================\nclass MS_SSIM_Loss(nn.Module):\n    \"\"\"\n    Multi-Scale Structural Similarity (MS-SSIM) Loss.\n    Implementation focusing only on structural similarity without L1 component.\n    \"\"\"\n    def __init__(self, gaussian_sigmas=[0.5, 1.0, 2.0, 4.0, 8.0],\n                 data_range=1.0,\n                 K=(0.01, 0.03),\n                 compensation=1.0,\n                 cuda_dev=0):\n        super(MS_SSIM_Loss, self).__init__()\n        \n        # Set the device attribute\n        self.device = torch.device(f'cuda:{cuda_dev}' if torch.cuda.is_available() else 'cpu')\n        \n        self.DR = data_range\n        self.C1 = (K[0] * data_range) ** 2\n        self.C2 = (K[1] * data_range) ** 2\n        self.pad = int(2 * gaussian_sigmas[-1])\n        self.compensation = compensation\n        filter_size = int(4 * gaussian_sigmas[-1] + 1)\n        \n        # Initialize Gaussian filters on the correct device\n        g_masks = torch.zeros((3 * len(gaussian_sigmas), 1, filter_size, filter_size), device=self.device)\n        for idx, sigma in enumerate(gaussian_sigmas):\n            # r0, g0, b0, r1, g1, b1, ...\n            g_masks[3 * idx + 0, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n            g_masks[3 * idx + 1, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n            g_masks[3 * idx + 2, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n        self.g_masks = g_masks  # Already on the correct device\n\n    def _fspecial_gauss_1d(self, size, sigma):\n        \"\"\"Create 1-D Gaussian kernel\"\"\"\n        coords = torch.arange(size, dtype=torch.float, device=self.device)\n        coords -= size // 2\n        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n        g /= g.sum()\n        return g.reshape(-1)\n\n    def _fspecial_gauss_2d(self, size, sigma):\n        \"\"\"Create 2-D Gaussian kernel\"\"\"\n        gaussian_vec = self._fspecial_gauss_1d(size, sigma)\n        return torch.outer(gaussian_vec, gaussian_vec)\n\n    def forward(self, x, y):\n        \"\"\"\n        Compute MS-SSIM loss between images x and y.\n        \n        Args:\n            x: Generated images tensor of shape (B, C, H, W)\n            y: Target/ground truth images tensor of shape (B, C, H, W)\n            \n        Returns:\n            Scalar tensor containing the loss value\n        \"\"\"\n        b, c, h, w = x.shape\n        mux = F.conv2d(x, self.g_masks, groups=3, padding=self.pad)\n        muy = F.conv2d(y, self.g_masks, groups=3, padding=self.pad)\n\n        mux2 = mux * mux\n        muy2 = muy * muy\n        muxy = mux * muy\n\n        sigmax2 = F.conv2d(x * x, self.g_masks, groups=3, padding=self.pad) - mux2\n        sigmay2 = F.conv2d(y * y, self.g_masks, groups=3, padding=self.pad) - muy2\n        sigmaxy = F.conv2d(x * y, self.g_masks, groups=3, padding=self.pad) - muxy\n\n        # l(j), cs(j) in MS-SSIM\n        l = (2 * muxy + self.C1) / (mux2 + muy2 + self.C1)  # [B, 15, H, W]\n        cs = (2 * sigmaxy + self.C2) / (sigmax2 + sigmay2 + self.C2)\n\n        # Final MS-SSIM calculation\n        lM = l[:, -1, :, :] * l[:, -2, :, :] * l[:, -3, :, :]\n        PIcs = cs.prod(dim=1)\n\n        # 1 - MS-SSIM as the loss (since MS-SSIM is a similarity measure)\n        loss_ms_ssim = 1 - lM * PIcs  # [B, H, W]\n        return self.compensation * loss_ms_ssim.mean()\n\n\nclass VGGPerceptualLoss(nn.Module):\n    \"\"\"\n    Perceptual loss using pretrained VGG19 features.\n    Compares high-level feature representations of images.\n    \"\"\"\n    def __init__(self, device, layer_cutoff=21):\n        super().__init__()\n        # Load a pretrained VGG19 and slice it\n        vgg19 = models.vgg19(pretrained=True).features\n        self.features = nn.Sequential(*[vgg19[i] for i in range(layer_cutoff)])\n        \n        # Freeze the parameters to avoid training them\n        for param in self.features.parameters():\n            param.requires_grad = False\n        \n        # Move the features to the specified device\n        self.features.to(device)\n        self.criterion = nn.MSELoss()\n\n    def forward(self, gen_img, target_img):\n        \"\"\"\n        Compute perceptual loss between generated and target images.\n        \n        Args:\n            gen_img: Generated image tensor\n            target_img: Target image tensor\n            \n        Returns:\n            Scalar tensor containing the perceptual loss\n        \"\"\"\n        # Ensure the images have the same dimensions\n        if gen_img.size() != target_img.size():\n            gen_img = F.interpolate(gen_img, size=target_img.shape[2:], \n                                  mode='bilinear', align_corners=False)\n        \n        # Extract features\n        gen_feat = self.features(gen_img)\n        tgt_feat = self.features(target_img)\n        \n        # Compute MSE loss between features\n        return self.criterion(gen_feat, tgt_feat)\n\n\n# ======================\n# Edge Loss\n# ======================\ndef edge_loss(pred, target, eps=1e-3):\n    \"\"\"\n    Edge loss using Laplacian filter.\n    Detects and preserves edges in the image.\n    \n    Args:\n        pred: Generated images tensor\n        target: Target images tensor\n        eps: Small epsilon to avoid numerical instability\n        \n    Returns:\n        Scalar tensor containing the edge loss\n    \"\"\"\n    laplacian_kernel = torch.tensor([[0.,  1., 0.],\n                                     [1., -4., 1.],\n                                     [0.,  1., 0.]], \n                                    dtype=torch.float32, device=pred.device).view(1, 1, 3, 3)\n    b, c, h, w = pred.shape\n    # Expand kernel for group conv on each channel\n    kernel_expanded = laplacian_kernel.expand(c, -1, -1, -1)\n\n    # Apply Laplacian filter to both prediction and target\n    pred_lap = F.conv2d(pred, kernel_expanded, padding=1, groups=c)\n    tgt_lap = F.conv2d(target, kernel_expanded, padding=1, groups=c)\n\n    # Calculate Charbonnier loss between the Laplacian responses\n    diff = pred_lap - tgt_lap\n    return torch.sqrt(diff * diff + eps * eps).mean()\n\n\n# ======================\n# Charbonnier Loss\n# ======================\ndef charbonnier_loss(pred, target, eps=0.001):\n    \"\"\"\n    Charbonnier loss - a differentiable variant of L1 loss.\n    More robust to outliers than MSE/L2 loss.\n    \n    Args:\n        pred: Generated images tensor\n        target: Target images tensor\n        eps: Small epsilon to avoid numerical instability\n        \n    Returns:\n        Scalar tensor containing the Charbonnier loss\n    \"\"\"\n    diff = pred - target\n    return torch.sqrt(diff * diff + eps * eps).mean()\n\n\n# ======================\n# Content Loss\n# ======================\ndef compute_content_loss(fake_imgs, clean_imgs, \n                         vgg_perceptual_loss,\n                         lambda_pixel=10,\n                         lambda_edge=0.5,\n                         lambda_vgg=1,\n                         eps_char=0.001,\n                         eps_edge=0.001):\n    \"\"\"\n    Combined content loss using pixel (Charbonnier), edge, and VGG perceptual losses.\n    \n    Args:\n        fake_imgs: Generated images tensor\n        clean_imgs: Target images tensor\n        vgg_perceptual_loss: VGG-based perceptual loss module\n        lambda_pixel: Weight for pixel loss\n        lambda_edge: Weight for edge loss\n        lambda_vgg: Weight for VGG perceptual loss\n        eps_char: Epsilon for Charbonnier loss\n        eps_edge: Epsilon for edge loss\n        \n    Returns:\n        Scalar tensor containing the weighted combination of losses\n    \"\"\"\n    # Pixel (Charbonnier) loss\n    pixel_l = charbonnier_loss(fake_imgs, clean_imgs, eps=eps_char)\n\n    # Edge loss\n    e_l = edge_loss(fake_imgs, clean_imgs, eps=eps_edge)\n\n    # Perceptual loss\n    p_l = vgg_perceptual_loss(fake_imgs, clean_imgs)\n\n    # Weighted sum\n    content_l = (lambda_pixel * pixel_l \n                 + lambda_edge * e_l \n                 + lambda_vgg * p_l)\n    return content_l\n\n\n# ======================\n# Total Variation Loss\n# ======================\ndef total_variation_loss(img):\n    \"\"\"\n    Total Variation loss for image smoothness.\n    Encourages spatial smoothness in generated images.\n    \n    Args:\n        img: Generated images tensor\n        \n    Returns:\n        Scalar tensor containing the TV loss\n    \"\"\"\n    tv_loss = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :])) + \\\n              torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]))\n    return tv_loss\n\n\n# ======================\n# Dual Denoising Loss\n# ======================\ndef dual_denoising_loss(clean_img, denoised_img, generator):\n    \"\"\"\n    Dual denoising loss - feeds denoised output back into generator.\n    Ensures the generator is robust to its own outputs.\n    \n    Args:\n        clean_img: Target clean images tensor\n        denoised_img: First-pass denoised images tensor\n        generator: The generator model\n        \n    Returns:\n        Scalar tensor containing the dual denoising loss\n    \"\"\"\n    # Run the denoised image through the generator again\n    re_denoised = generator(denoised_img.detach())\n    return F.smooth_l1_loss(re_denoised, clean_img)\n\n\n# ======================\n# Multiscale Adversarial Loss\n# ======================\ndef multiscale_adversarial_loss(discriminator_outputs, is_real=True):\n    \"\"\"\n    Calculates adversarial loss across multiple scales of the discriminator.\n    Each scale contributes individually to the loss, preserving detailed feedback.\n    \n    Args:\n        discriminator_outputs: List of tensors from different scales of the discriminator\n        is_real: True for real samples, False for fake samples\n    \n    Returns:\n        Tuple of (total loss, list of individual scale losses)\n    \"\"\"\n    scale_losses = []\n    weights = []\n    \n    for i, output in enumerate(discriminator_outputs):\n        # Higher weight for smaller scales to emphasize texture details\n        scale_weight = 1.0 / (i + 1)\n        weights.append(scale_weight)\n        \n        if is_real:\n            # For real samples, maximize output (WGAN approach)\n            scale_losses.append(-output.mean() * scale_weight)\n        else:\n            # For fake samples, minimize output (WGAN approach)\n            scale_losses.append(output.mean() * scale_weight)\n    \n    # Normalize by sum of weights\n    total_weight = sum(weights)\n    total_loss = sum(scale_losses) / total_weight\n    \n    return total_loss, scale_losses\n\n\n# ======================\n# Multiscale Gradient Penalty\n# ======================\ndef gradient_penalty_multiscale(critic, noisy_imgs, real_clean, fake_denoised, device, lambda_gp=10):\n    \"\"\"\n    WGAN-GP gradient penalty that works with multi-scale discriminator outputs.\n    \n    Args:\n        critic: Multi-scale discriminator\n        noisy_imgs: Noisy input images\n        real_clean: Ground truth clean images\n        fake_denoised: Generator output images\n        device: Computation device\n        lambda_gp: Gradient penalty weight\n    \n    Returns:\n        Gradient penalty loss\n    \"\"\"\n    batch_size = real_clean.size(0)\n    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n    \n    # Interpolate between real and fake samples\n    interpolated = (alpha * real_clean + (1 - alpha) * fake_denoised).requires_grad_(True)\n    \n    # Get discriminator output for interpolated samples\n    d_interpolated = critic(noisy_imgs, interpolated)\n    \n    # We need to calculate gradient penalty for each scale\n    total_gp = 0\n    weights = []\n    \n    for i, output in enumerate(d_interpolated):\n        # Higher weight for smaller scales\n        scale_weight = 1.0 / (i + 1)\n        weights.append(scale_weight)\n        \n        fake = torch.ones_like(output, requires_grad=False, device=device)\n        \n        # Get gradients\n        gradients = torch.autograd.grad(\n            outputs=output,\n            inputs=interpolated,\n            grad_outputs=fake,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True,\n        )[0]\n        \n        # Flatten gradients\n        gradients = gradients.view(batch_size, -1)\n        \n        # Calculate gradient penalty for this scale\n        gradient_norm = gradients.norm(2, dim=1)\n        scale_gp = ((gradient_norm - 1) ** 2).mean() * scale_weight\n        \n        total_gp += scale_gp\n    \n    # Normalize by sum of weights\n    total_weight = sum(weights)\n    return (total_gp / total_weight) * lambda_gp\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Traning Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim_sk\n\n# Assuming all models, loss functions, etc. are defined above\n\n# ===============\n#  Hyperparameters\n# ===============\nepochs = 100\nn_critic = 7\nlambda_gp = 10\nsmoothing_lambda = 0.09\nlambda_ssim = 0.5\nlambda_dd = 0.5       # Dual denoising weight\nperceptual_lambda = 0.1\nlambda_char = 1.0\n\nbest_ssim = 0.0\nbest_psnr = 0.0\nloss_history = []\n\n# Loss weights\nlambda_adv = 0.01           # Adversarial loss for generator\nlambda_tv = 0.01            # Total variation\nlambda_dual = 0.1           # Dual denoising\nlambda_content = 1.0        # Overall content loss weight\n\n# Inside content loss\nlambda_pixel = 10.0         # Charbonnier/pixel loss\nlambda_edge = 0.5           # Edge loss (if used)\nlambda_vgg = 1.0            # VGG perceptual\n\n# Charbonnier/edge epsilon\ncharbonnier_eps = 0.001     # from text references\n\n# ===============\n#  Model & Optim\n# ===============\n# Initialize models\ngenerator = FractalDenoiseGenerator(\n    patch_sizes=[16, 4],  # or whatever patch scales you want\n    overlap=2,            # or 0 if you prefer no overlap\n    in_channels=3,        # defaults to 3\n    hidden_channels=32    # defaults to 32\n).to(device)\ncritic = Discriminator().to(device)\n\n# If you have multiple GPUs:\n# if torch.cuda.device_count() > 1:\n#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n#     generator = nn.DataParallel(generator)\n#     critic = nn.DataParallel(critic)\n\noptimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.9, 0.999))\noptimizer_D = optim.Adam(critic.parameters(), lr=2e-4, betas=(0.9, 0.999))\n\n# (Optional) Resume from a previous checkpoint\n# resume_training = True\n# checkpoint_path = \"/kaggle/input/12-epoch-lfg/best_generator_ssim.pth\"\n# if resume_training:\n#     start_epoch, loss_history = load_checkpoint(\n#         generator, critic, optimizer_G, optimizer_D,\n#         filepath=checkpoint_path, \n#         device=device\n#     )\n#     print(f\"Resuming training from epoch {start_epoch+1}\")\n# else:\n#     start_epoch = 0\n\n# Create MSSSIM loss and VGG perceptual loss\nmsssim_loss = MS_SSIM_Loss(cuda_dev=0).to(device)\nvgg_perceptual_loss = VGGPerceptualLoss(device=device, layer_cutoff=21)\n\n\ndef calculate_metrics(generator, dataloader, device, win_size=3):\n    generator.eval()\n    psnr_values = []\n    ssim_values = []\n\n    with torch.no_grad():\n        for i, (noisy_imgs, clean_imgs) in enumerate(dataloader):\n            noisy_imgs = noisy_imgs.to(device)\n            clean_imgs = clean_imgs.cpu().numpy()\n\n            denoised_imgs = generator(noisy_imgs).cpu().numpy()\n\n            # Calculate PSNR and SSIM for each image\n            for denoised, clean in zip(denoised_imgs, clean_imgs):\n                # Convert to HWC\n                denoised = np.transpose(denoised, (1, 2, 0))\n                clean = np.transpose(clean, (1, 2, 0))\n\n                psnr_value = psnr(clean, denoised, data_range=1)\n                ssim_value = ssim_sk(clean, denoised, data_range=1, win_size=win_size, multichannel=True)\n\n                psnr_values.append(psnr_value)\n                ssim_values.append(ssim_value)\n\n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n    return avg_psnr, avg_ssim\n\n\n# ==================\n#     Training\n# ==================\nstart_epoch = 0  # or from checkpoint if resuming\n\n# Track metrics for plotting\ncritic_losses = []\ngen_losses = []\ncontent_losses = []\nmsssim_losses = []\ntv_losses = []\ndual_losses = []\npsnr_metrics = []\nssim_metrics = []\n\n########################\n#   TRAINING LOOP\n########################\nfor epoch in range(start_epoch + 1, epochs + 1):\n    generator.train()\n    critic.train()\n    \n    # Track batch losses for epoch average\n    epoch_critic_loss = 0\n    epoch_gen_adv_loss = 0\n    epoch_content_loss = 0\n    epoch_msssim_loss = 0\n    epoch_tv_loss = 0\n    epoch_dual_loss = 0\n    batch_count = 0\n\n    for i, (noisy_imgs, clean_imgs) in enumerate(train_loader):\n        noisy_imgs = noisy_imgs.to(device)\n        clean_imgs = clean_imgs.to(device)\n        batch_count += 1\n\n        ################################################\n        # (1) TRAIN CRITIC (WGAN-GP)\n        ################################################\n        for _ in range(n_critic):\n            fake_imgs = generator(noisy_imgs)\n            optimizer_D.zero_grad()\n\n            # Get real and fake scores (now returns lists)\n            real_outputs = critic(noisy_imgs, clean_imgs)\n            fake_outputs = critic(noisy_imgs, fake_imgs.detach())\n            \n            # Calculate losses for each scale\n            real_loss, real_scale_losses = multiscale_adversarial_loss(real_outputs, is_real=True)\n            fake_loss, fake_scale_losses = multiscale_adversarial_loss(fake_outputs, is_real=False)\n            \n            # Calculate multi-scale gradient penalty\n            gp = gradient_penalty_multiscale(\n                critic, noisy_imgs, clean_imgs, fake_imgs.detach(), device, lambda_gp=lambda_gp\n            )\n            \n            # Combined critic loss\n            critic_loss = real_loss + fake_loss + gp\n\n            critic_loss.backward()\n            optimizer_D.step()\n\n        ################################################\n        # (2) TRAIN GENERATOR\n        ################################################\n        optimizer_G.zero_grad()\n\n        # Re-generate fakes (after critic update)\n        fake_imgs = generator(noisy_imgs)\n        \n        # Get discriminator outputs for fake images\n        fake_outputs = critic(noisy_imgs, fake_imgs)\n        \n        # Calculate adversarial loss for generator\n        gen_adv_loss, gen_scale_losses = multiscale_adversarial_loss(fake_outputs, is_real=True)\n        gen_adv_loss_scaled = lambda_adv * gen_adv_loss\n\n        # Content loss (Charbonnier + Edge + VGG)\n        cont_l = compute_content_loss(\n            fake_imgs, clean_imgs,\n            vgg_perceptual_loss=vgg_perceptual_loss,\n            lambda_pixel=lambda_pixel,\n            lambda_edge=lambda_edge,\n            lambda_vgg=lambda_vgg,\n            eps_char=charbonnier_eps,\n            eps_edge=charbonnier_eps\n        )\n        content_loss_scaled = lambda_content * cont_l\n\n        # MS-SSIM loss (replaced MS-SSIM-L1)\n        msssim_val = msssim_loss(fake_imgs, clean_imgs)\n        msssim_scaled = lambda_ssim * msssim_val\n\n        # Total Variation loss\n        tv_l = total_variation_loss(fake_imgs)\n        tv_loss_scaled = lambda_tv * tv_l\n\n        # Dual Denoising loss\n        dd_l = dual_denoising_loss(clean_imgs, fake_imgs.detach(), generator)\n        dd_loss_scaled = lambda_dual * dd_l\n\n        # Combine all generator losses\n        total_gen_loss = (gen_adv_loss_scaled\n                         + content_loss_scaled\n                         + msssim_scaled\n                         + tv_loss_scaled\n                         + dd_loss_scaled)\n\n        total_gen_loss.backward()\n        optimizer_G.step()\n        \n        # Track losses for this epoch\n        epoch_critic_loss += critic_loss.item()\n        epoch_gen_adv_loss += gen_adv_loss.item()\n        epoch_content_loss += cont_l.item()\n        epoch_msssim_loss += msssim_val.item()\n        epoch_tv_loss += tv_l.item()\n        epoch_dual_loss += dd_l.item()\n        \n        # Print batch progress\n        # if (i + 1) % 10 == 0:\n        #     print(f\"[Epoch {epoch}/{epochs}] [Batch {i+1}/{len(train_loader)}] \"\n        #           f\"D Loss: {critic_loss.item():.4f}, G Loss: {total_gen_loss.item():.4f}\")\n\n    # Calculate epoch averages\n    epoch_critic_loss /= batch_count\n    epoch_gen_adv_loss /= batch_count\n    epoch_content_loss /= batch_count\n    epoch_msssim_loss /= batch_count\n    epoch_tv_loss /= batch_count\n    epoch_dual_loss /= batch_count\n    \n    # End-of-epoch logging\n    print(f\"[Epoch {epoch}/{epochs}] \"\n          f\"Critic Loss: {epoch_critic_loss:.4f} \"\n          f\"Gen Adv Loss: {epoch_gen_adv_loss:.4f} \"\n          f\"Content: {epoch_content_loss:.4f} \"\n          f\"MS-SSIM: {epoch_msssim_loss:.4f} \"\n          f\"TV: {epoch_tv_loss:.4f} \"\n          f\"Dual: {epoch_dual_loss:.4f}\")\n\n    # Validation and checkpoint saving...\n    avg_psnr, avg_ssim = calculate_metrics(generator, val_loader, device, win_size=3)\n    print(f\"Validation -- PSNR: {avg_psnr:.2f}  SSIM: {avg_ssim:.4f}\")\n    \n    # Store metrics for plotting\n    critic_losses.append(epoch_critic_loss)\n    gen_losses.append(epoch_gen_adv_loss)\n    content_losses.append(epoch_content_loss)\n    msssim_losses.append(epoch_msssim_loss)\n    tv_losses.append(epoch_tv_loss)\n    dual_losses.append(epoch_dual_loss)\n    psnr_metrics.append(avg_psnr)\n    ssim_metrics.append(avg_ssim)\n\n    # Store in loss history\n    loss_history.append((epoch,\n                         epoch_critic_loss,\n                         epoch_gen_adv_loss,\n                         epoch_content_loss,\n                         epoch_msssim_loss,\n                         epoch_tv_loss,\n                         epoch_dual_loss,\n                         avg_psnr,\n                         avg_ssim))\n    \n    # Save checkpoints\n    if avg_ssim > best_ssim:\n        best_ssim = avg_ssim\n        save_best_checkpoint(generator, critic, optimizer_G, optimizer_D,\n                             epoch, loss_history, best_ssim, best_psnr,\n                             'best_generator_ssim.pth')\n    if avg_psnr > best_psnr:\n        best_psnr = avg_psnr\n        save_best_checkpoint(generator, critic, optimizer_G, optimizer_D,\n                             epoch, loss_history, best_ssim, best_psnr,\n                             'best_generator_psnr.pth')\n    \n    # Visualize sample results periodically\n    if epoch % 20 == 0:\n        show_images_from_batches(generator, val_loader, device, num_images=15)\n\nprint(\"Training complete!\")\n\n# ==================\n#   Plot Losses\n# ==================\nepochs_range = range(1, epochs + 1)\n\n# Plot loss curves\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Generator and Discriminator losses\nplt.subplot(3, 2, 1)\nplt.plot(epochs_range, critic_losses, 'b-', label='Critic Loss')\nplt.plot(epochs_range, gen_losses, 'r-', label='Generator Adv Loss')\nplt.title('Adversarial Losses')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot 2: Content Loss\nplt.subplot(3, 2, 2)\nplt.plot(epochs_range, content_losses, 'g-', label='Content Loss')\nplt.title('Content Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot 3: MS-SSIM Loss\nplt.subplot(3, 2, 3)\nplt.plot(epochs_range, msssim_losses, 'c-', label='MS-SSIM Loss')\nplt.title('MS-SSIM Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot 4: TV and Dual Losses\nplt.subplot(3, 2, 4)\nplt.plot(epochs_range, tv_losses, 'm-', label='TV Loss')\nplt.plot(epochs_range, dual_losses, 'y-', label='Dual Denoising Loss')\nplt.title('Regularization Losses')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot 5: PSNR\nplt.subplot(3, 2, 5)\nplt.plot(epochs_range, psnr_metrics, 'b-', label='PSNR')\nplt.title('PSNR Metric')\nplt.xlabel('Epochs')\nplt.ylabel('PSNR (dB)')\nplt.legend()\nplt.grid(True)\n\n# Plot 6: SSIM\nplt.subplot(3, 2, 6)\nplt.plot(epochs_range, ssim_metrics, 'r-', label='SSIM')\nplt.title('SSIM Metric')\nplt.xlabel('Epochs')\nplt.ylabel('SSIM')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig('training_metrics.png', dpi=300)\nplt.show()\n\n# ==================\n# Plot Individual Metrics\n# ==================\n\n# PSNR over epochs\nplt.figure(figsize=(12, 6))\nplt.plot(epochs_range, psnr_metrics, 'b-', linewidth=2)\nplt.title('PSNR Over Training Epochs', fontsize=16)\nplt.xlabel('Epochs', fontsize=14)\nplt.ylabel('PSNR (dB)', fontsize=14)\nplt.grid(True)\nplt.savefig('psnr_progress.png', dpi=300)\nplt.show()\n\n# SSIM over epochs\nplt.figure(figsize=(12, 6))\nplt.plot(epochs_range, ssim_metrics, 'r-', linewidth=2)\nplt.title('SSIM Over Training Epochs', fontsize=16)\nplt.xlabel('Epochs', fontsize=14)\nplt.ylabel('SSIM', fontsize=14)\nplt.grid(True)\nplt.savefig('ssim_progress.png', dpi=300)\nplt.show()\n\n# Combined MS-SSIM and Content loss\nplt.figure(figsize=(12, 6))\nplt.plot(epochs_range, msssim_losses, 'c-', label='MS-SSIM Loss', linewidth=2)\nplt.plot(epochs_range, content_losses, 'g-', label='Content Loss', linewidth=2)\nplt.title('Perceptual Quality Losses', fontsize=16)\nplt.xlabel('Epochs', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.savefig('perceptual_losses.png', dpi=300)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}